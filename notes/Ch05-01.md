# Update Stategies and Prometheus

There are multiple update/deployment/release strategies. We will focus on Rolling update and Canary release.

### Rolling Update

K8s starts a rolling update when we change the image. Every pod is updated sequentially, and the app will be available during the update. If the image doesn't work, the update will automatically stop.

However, what if the app runs, but there's a bug that prevents it from working correctly? We can use a **ReadinessProbe** to check if a pod is ready for traffic.

For example:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flaky-update-dep
spec:
  replicas: 4
  selector:
    matchLabels:
      app: flaky-update
  template:
    metadata:
      labels:
        app: flaky-update
    spec:
      containers:
        - name: flaky-update
          image: mluukkai/dwk-app8:v1
          readinessProbe:
            initialDelaySeconds: 10 # Initial delay until the readiness is tested
            periodSeconds: 5 # How often to test
            httpGet:
              path: /healthz
              port: 3541
```

This app will have an endpoint `/healthz` that returns a 200 status code if the app is ready, or a 500 if it is not. The initial delay is to give the app time to start up, and the period is how often to check. If a pod fails the readiness check, it will be running, but it will not be READY.

Another importante Probe is the **LivenessProbe**. Here, if the check fails, the container will be restarted:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flaky-update-dep
spec:
  replicas: 4
  selector:
    matchLabels:
      app: flaky-update
  template:
    metadata:
      labels:
        app: flaky-update
    spec:
      containers:
        - name: flaky-update
          image: mluukkai/dwk-app8:v1
          readinessProbe:
            initialDelaySeconds: 10 # Initial delay until the readiness is tested
            periodSeconds: 5 # How often to test
            httpGet:
              path: /healthz
              port: 3541
          livenessProbe:
            initialDelaySeconds: 20 # Initial delay until the liveness is tested
            periodSeconds: 5 # How often to test
            httpGet:
              path: /healthz
              port: 3541
```

If we need to undo a deployment, we can use the `kubectl rollout undo` command: `kubectl rollout undo deployment flaky-update-dep`

Another useful Probe if the app is slow to start is the **StartupProbe**. It can be used to delay the liveness probe.

### Canary Release

Sometimes we need to do partial releases. Canary release is the term used to describe a release strategy in which we deploy a new version of an app to a small subset of users, and gradually increase the number of users that get the new version.

This strategy is not provided by default in k8s. We will use Argo Rollouts to implement this strategy.

`kubectl create namespace argo-rollouts`
`kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml`

Now we can use a new resource called **Rollout**. It will replace the Deployment:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: flaky-update-dep
spec:
  replicas: 4
  selector:
    matchLabels:
      app: flaky-update
  strategy:
    canary:
      steps:
        - setWeight: 25
        - pause:
            duration: 30s
        - setWeight: 50
        - pause:
            duration: 30s
  template:
    metadata:
      labels:
        app: flaky-update
    spec:
      containers:
        - name: flaky-update
          image: mluukkai/dwk-app8:v1
          readinessProbe:
            initialDelaySeconds: 10 # Initial delay until the readiness is tested
            periodSeconds: 5 # How often to test
            httpGet:
              path: /healthz
              port: 3541
          livenessProbe:
            initialDelaySeconds: 20 # Initial delay until the liveness is tested
            periodSeconds: 5 # How often to test
            httpGet:
              path: /healthz
              port: 3541

---
# the service definition could be put here
```

This strategy will move 25% (setWeight) of the pods to the new version, it will wait 30 seconds, and then move 50% (setWeight) of the pods to the new version, and finally wait 30 seconds and move the remaining pods to the new version.

Another resource that Argo Rollouts provides is the **AnalysisTemplate**. It will define a test that doesn't let broken version through.

```yaml
# ... Previous Rollout definition
strategy:
  canary:
    steps:
      - setWeight: 50
      - analysis:
          templates:
            - templateName: restart-rate
# ...
```

Now we will use Prometheus to define how the analysis should be done. In our case, we will query the state of the deployment, and compare it to a preset value. The `analysistemplate.yaml` file will look like this:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: AnalysisTemplate
metadata:
  name: restart-rate
spec:
  metrics:
    - name: restart-rate
      initialDelay: 2m
      successCondition: result < 2
      provider:
        prometheus:
          address: http://kube-prometheus-stack-1602-prometheus.prometheus.svc.cluster.local:9090 # DNS name for my Prometheus, find yours with kubectl describe svc ...
          query: |
            scalar(
              sum(kube_pod_container_status_restarts_total{namespace="default", container="flaky-update"}) -
              sum(kube_pod_container_status_restarts_total{namespace="default", container="flaky-update"} offset 2m)
            )
```

If there more than 2 restarts in 2 minutes, the analysis will fail. Note that the AnalysisTemplate is not dependant on Prometheus, it could use a different source, but Prometheus is the most common one.

We can watch the Rollout with `kubectl argo rollouts get rollout flaky-update-dep --watch`

### Other Deployment Strategies

K8s supports Recreate strategy which takes down the previous pods and replaces everything with the updated one. Argo Rollouts support BlueGreen strategy, in which a new version is run side by side to the new one, but traffic is switched between the two at a certain point.
