# GitOps

In a push deployment:

1. The source code is pushed to a repository.
2. This triggers a CI/CD pipeline.
3. The CI/CD runs tests, builds and pushes the image to a registry, and deploys the image.

In a pull deployment, the setup is reversed: we pull the image from the registry, and deploy it. It will still be tested and built by the CI/CD, we simply relieve the CI/CD of the deployment step.

GitOps is all about this reversal and promotes good practices. We do this by maintaining the state of the cluster in a Git repository, thereby handling not only app deployments, but also the configuration of the cluster.

ArgoCD is the current leading tool for GitOps in k8s. In the end, our workflow will look like this:

1. Developer pushes code to a Git repository.
2. CI/CD starts running. (in our case, GitHub Actions)
3. CI/CD builds and pushes new image and commits edit to the "release" branch. (in our case, main)
4. ArgoCD takes the state described in the release branch and sets it as the state of our cluster.

Install ArgoCD:
`kubectl create namespace argocd`
`kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml`

Open access to it, we will use the LoadBalancer:
`kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'`

Get the initial admin password:
`kubectl get -n argocd secrets argocd-initial-admin-secret -o yaml`

Log in at `https://<external-ip>/`

We can deploy a new app using ArgoCD:

1. Click New App. We will use a manual sync policy.
2. Since our kustomization.yaml is in the root of the repository, we can use the root directory as the 'path'.
3. Sync, if anything goes wrong, we can fix the kustomization.yaml and sync again. If we use automatic sync, ArgoCD will do this for us.

Besides the services, deployments and pods, the app config tree will also show some other objects: a ReplicaSet ensures there is always a stable set of running pods. It defines the number of replicas, and if a pod dies, it will create a new one to replace it. Usually we will define ReplicaSets within Deployments.

To deploy a new version of our app:

1. Create a new image, with possibly a new tag.
2. Change the kustomization.yaml to point to the new image.
3. Push the changes to the repository.

This can be done manually, but we will automate it with a GitHub Actions workflow.

Step one is already known.
Step two will be done like this: `kustomize edit set image PROJECT/NAME=node:20`. This will change the image in the kustomization.yaml file.
Now we can commit the changes to the repository using GitHub Actions:

```yaml
name: Build and publish application

on:
  push:

jobs:
  build-publish:
    name: Build, Push, Release
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      # tag image with the GitHub SHA to get a unique tag
      - name: Build and publish backend
        run: |-
          docker build --tag "mluukkai/dwk1:$GITHUB_SHA" .
          docker push "mluukkai/dwk1:$GITHUB_SHA"

      - name: Set up Kustomize
        uses: imranismail/setup-kustomize@v2

      - name: Use right image
        run: kustomize edit set image PROJECT/IMAGE=mluukkai/dwk1:$GITHUB_SHA

      - name: commit kustomization.yaml to GitHub
        uses: EndBug/add-and-commit@v9
        with:
          add: "kustomization.yaml"
          message: New version released ${{ github.sha }}
```

The workflow will have to have permission to write to the repository.

### More Environments

If the app uses many different environments, the recommended approach when using Kustomize is to define a common base, and then environment-specific overlays.

So we might have a base/deployment.yaml that looks like this:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-dep
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp
          image: PROJECT/IMAGE
```

And a base/kustomization.yaml that looks like this:

```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - deployment.yaml
```

Then we could have a overlays/prod/deployment.yaml that looks like this:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dep
spec:
  replicas: 3
```

It refines the base by changing the replicas to 3. Only parts that are different are defined.

We would then have a overlays/prod/kustomization.yaml that looks like this:

```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - ./../../base
patches:
  - path: deployment.yaml

namePrefix: prod-

images:
  - name: PROJECT/IMAGE
    newName: nginx:1.25-bookworm
```

It simply refers to the base and patches it with the overlays/prod/deployment.yaml file.

### Defining the Whole with YAML

So far we've used ArgoCD UI to define apps, but we could use Argo CLI, or to do a declarative approach and do the definition with Application custom resource, which is a k8s resource object representing a deployed app instance. It defines the source, the Git repository, and the destination, the cluster.

For the production environment above, we could define an application.yaml like this:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: myapp-production
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/mluukkai/gitopstest
    path: overlays/prod
    targetRevision: HEAD
  destination:
    server: https://kubernetes.default.svc
    namespace: default
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

We then apply it with `kubectl apply -n argocd -f application.yaml`.
