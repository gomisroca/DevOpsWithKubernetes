# Messaging Systems

Message queues are a way for services to communicate. Instead of all services talking to each other, they talk to a queue, and the queue talks to the other services.

We will mainly use NATS. In NATS apps communicate by sending and receiving messages. These messages have a subject. The sender publishes the message with a subject, and the receiver subscribes to the subject to get the messages. There can be multiple subscribers to the same subject, or we can use a queue, where only one subscriber can get the message.

NATS's basic functionality is "at most, once": if no one is listening to a subject, the message will be lost. We can also use Jetstream to achieve "at least once" or "exactly once" delivery.

For example, we can have a dataset of 100000 JSON objects we want to process and save. Our app will have 3 parts:

- Fetcher, which will fetch the raw data and pass it to NATS.
- Mapper, which will process the data and then send it back to NATS.
- Saver, which will receive the processed data and save it to a database.

The app will use 4 NATS subjects:

- mapper_status: The mapper will send a message to the fetcher when it is ready to process the data.
- mapper_data: The fetcher will send the data to the mapper.
- saver_data: The mapper will send the processed data to the saver.
- processed_data: The saver will inform the fetcher about which chunks of data have been processed.

Before we start, we need to install NATS via Helm:
`helm install --set auth.enabled=false my-nats oci://registry-1.docker.io/bitnamicharts/nats`

Now we can create the deployment.yaml, which will pass the NATS URL to pods:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mapper-dep
spec:
  replicas: 10
  selector:
    matchLabels:
      app: mapper
  template:
    metadata:
      labels:
        app: mapper
    spec:
      containers:
        - name: mapper
          image: jakousa/dwk-app9-mapper:0bcd6794804c367684a9a79bb142bb4455096974
          env:
            - name: NATS_URL
              value: nats://my-nats:4222
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fetcher-dep
spec:
  replicas: 1
  selector:
    matchLabels:
      app: fetcher
  template:
    metadata:
      labels:
        app: fetcher
    spec:
      containers:
        - name: fetcher
          image: jakousa/dwk-app9-fetcher:0bcd6794804c367684a9a79bb142bb4455096974
          env:
            - name: NATS_URL
              value: nats://my-nats:4222
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: saver-dep
spec:
  replicas: 1
  selector:
    matchLabels:
      app: saver
  template:
    metadata:
      labels:
        app: saver
    spec:
      containers:
        - name: saver
          image: jakousa/dwk-app9-saver:0bcd6794804c367684a9a79bb142bb4455096974
          env:
            - name: NATS_URL
              value: nats://my-nats:4222
```

We can monitor the status of NATS with `kubectl port-forward my-nats-0 8222:8222` and going to `http://localhost:8222`.

Next, we want to be able to use Prometheus for monitoring. We need to enable the Prometheus exporter, which will expose the metrics on port 7777: `helm upgrade --set metrics.enabled=true,auth.enabled=false my-nats oci://registry-1.docker.io/bitnamicharts/nats`. We can then access the metrics with `kubectl port-forward --namespace default svc/my-nats-metrics 7777:7777` and going to `http://localhost:7777/metrics`.

We will need to create a ServiceMonitor to connect Prometheus to the exporter, we can do this with Helm, but since we are using so many parameters, we will create a config file `myvalues.yaml`:

```yaml
auth:
  enabled: false

metrics:
  enabled: true
  serviceMonitor:
    enabled: true
    namespace: prometheus
```

Now we can just run `helm upgrade -f myvalues.yaml my-nats oci://registry-1.docker.io/bitnamicharts/nats`

We need a label for our config so Prometheus knows to listen to NATS. We can use the label from the existing ServiceMonitor, which we will find with `kubectl -n prometheus get prometheus` and `kubectl describe prometheus -n prometheus kube-prometheus-stack-1744-prometheus`. We will get something like `release: kube-prometheus-stack-1744138938`. Finally, we can attach the label with `kubectl label servicemonitors.monitoring.coreos.com -n prometheus my-nats-metrics release=kube-prometheus-stack-1744138938`. We can check if Prometheus has access to the data with `kubectl -n prometheus port-forward prometheus-kube-prometheus-stack-1744-prometheus-0 9090`. The NATS metrics ServiceMonitor should be amongst the targets.

Lastly, we will add a Grafana dashboard to visualize the metrics: `kubectl -n prometheus port-forward kube-prometheus-stack-1602180058-grafana-59cd48d794-4459m 3000`. We can use "Import Dashboard" to paste a JSON file which will give us a simple dashboard.
